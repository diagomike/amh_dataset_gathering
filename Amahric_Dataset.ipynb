{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vzmePqKte2dQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "af63e782-cbf6-4ebb-81d4-82eef86234e5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "!pip install -qq git+https://github.com/m-bain/whisperx.git\n",
        "# !pip install -qq https://github.com/pyannote/pyannote-audio/archive/refs/heads/develop.zip\n",
        "# !pip install -qq ipython==7.34.0"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title load WhisperX VAD\n",
        "import hashlib\n",
        "import os\n",
        "import urllib\n",
        "from typing import Callable, Optional, Text, Union\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "from pyannote.audio import Model\n",
        "from pyannote.audio.core.io import AudioFile\n",
        "from pyannote.audio.pipelines import VoiceActivityDetection\n",
        "from pyannote.audio.pipelines.utils import PipelineModel\n",
        "from pyannote.core import Annotation, Segment, SlidingWindowFeature\n",
        "from tqdm import tqdm\n",
        "\n",
        "class SegmentX:\n",
        "    def __init__(self, start, end, speaker=None):\n",
        "        self.start = start\n",
        "        self.end = end\n",
        "        self.speaker = speaker\n",
        "\n",
        "VAD_SEGMENTATION_URL = \"https://whisperx.s3.eu-west-2.amazonaws.com/model_weights/segmentation/0b5b3216d60a2d32fc086b47ea8c67589aaeb26b7e07fcbe620d6d0b83e209ea/pytorch_model.bin\"\n",
        "\n",
        "def load_vad_model(device, vad_onset=0.500, vad_offset=0.363, use_auth_token=None, model_fp=None):\n",
        "    model_dir = torch.hub._get_torch_home()\n",
        "    os.makedirs(model_dir, exist_ok = True)\n",
        "    if model_fp is None:\n",
        "        model_fp = os.path.join(model_dir, \"whisperx-vad-segmentation.bin\")\n",
        "    if os.path.exists(model_fp) and not os.path.isfile(model_fp):\n",
        "        raise RuntimeError(f\"{model_fp} exists and is not a regular file\")\n",
        "\n",
        "    if not os.path.isfile(model_fp):\n",
        "        with urllib.request.urlopen(VAD_SEGMENTATION_URL) as source, open(model_fp, \"wb\") as output:\n",
        "            with tqdm(\n",
        "                total=int(source.info().get(\"Content-Length\")),\n",
        "                ncols=80,\n",
        "                unit=\"iB\",\n",
        "                unit_scale=True,\n",
        "                unit_divisor=1024,\n",
        "            ) as loop:\n",
        "                while True:\n",
        "                    buffer = source.read(8192)\n",
        "                    if not buffer:\n",
        "                        break\n",
        "\n",
        "                    output.write(buffer)\n",
        "                    loop.update(len(buffer))\n",
        "\n",
        "    model_bytes = open(model_fp, \"rb\").read()\n",
        "    if hashlib.sha256(model_bytes).hexdigest() != VAD_SEGMENTATION_URL.split('/')[-2]:\n",
        "        raise RuntimeError(\n",
        "            \"Model has been downloaded but the SHA256 checksum does not not match. Please retry loading the model.\"\n",
        "        )\n",
        "\n",
        "    vad_model = Model.from_pretrained(model_fp, use_auth_token=use_auth_token)\n",
        "    hyperparameters = {\"onset\": vad_onset,\n",
        "                    \"offset\": vad_offset,\n",
        "                    \"min_duration_on\": 0.1,\n",
        "                    \"min_duration_off\": 0.1}\n",
        "    vad_pipeline = VoiceActivitySegmentation(segmentation=vad_model, device=torch.device(device))\n",
        "    vad_pipeline.instantiate(hyperparameters)\n",
        "\n",
        "    return vad_pipeline\n",
        "\n",
        "class Binarize:\n",
        "    \"\"\"Binarize detection scores using hysteresis thresholding, with min-cut operation\n",
        "    to ensure not segments are longer than max_duration.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    onset : float, optional\n",
        "        Onset threshold. Defaults to 0.5.\n",
        "    offset : float, optional\n",
        "        Offset threshold. Defaults to `onset`.\n",
        "    min_duration_on : float, optional\n",
        "        Remove active regions shorter than that many seconds. Defaults to 0s.\n",
        "    min_duration_off : float, optional\n",
        "        Fill inactive regions shorter than that many seconds. Defaults to 0s.\n",
        "    pad_onset : float, optional\n",
        "        Extend active regions by moving their start time by that many seconds.\n",
        "        Defaults to 0s.\n",
        "    pad_offset : float, optional\n",
        "        Extend active regions by moving their end time by that many seconds.\n",
        "        Defaults to 0s.\n",
        "    max_duration: float\n",
        "        The maximum length of an active segment, divides segment at timestamp with lowest score.\n",
        "    Reference\n",
        "    ---------\n",
        "    Gregory Gelly and Jean-Luc Gauvain. \"Minimum Word Error Training of\n",
        "    RNN-based Voice Activity Detection\", InterSpeech 2015.\n",
        "\n",
        "    Modified by Max Bain to include WhisperX's min-cut operation\n",
        "    https://arxiv.org/abs/2303.00747\n",
        "\n",
        "    Pyannote-audio\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        onset: float = 0.5,\n",
        "        offset: Optional[float] = None,\n",
        "        min_duration_on: float = 0.0,\n",
        "        min_duration_off: float = 0.0,\n",
        "        pad_onset: float = 0.0,\n",
        "        pad_offset: float = 0.0,\n",
        "        max_duration: float = float('inf')\n",
        "    ):\n",
        "\n",
        "        super().__init__()\n",
        "\n",
        "        self.onset = onset\n",
        "        self.offset = offset or onset\n",
        "\n",
        "        self.pad_onset = pad_onset\n",
        "        self.pad_offset = pad_offset\n",
        "\n",
        "        self.min_duration_on = min_duration_on\n",
        "        self.min_duration_off = min_duration_off\n",
        "\n",
        "        self.max_duration = max_duration\n",
        "\n",
        "    def __call__(self, scores: SlidingWindowFeature) -> Annotation:\n",
        "        \"\"\"Binarize detection scores\n",
        "        Parameters\n",
        "        ----------\n",
        "        scores : SlidingWindowFeature\n",
        "            Detection scores.\n",
        "        Returns\n",
        "        -------\n",
        "        active : Annotation\n",
        "            Binarized scores.\n",
        "        \"\"\"\n",
        "\n",
        "        num_frames, num_classes = scores.data.shape\n",
        "        frames = scores.sliding_window\n",
        "        timestamps = [frames[i].middle for i in range(num_frames)]\n",
        "\n",
        "        # annotation meant to store 'active' regions\n",
        "        active = Annotation()\n",
        "        for k, k_scores in enumerate(scores.data.T):\n",
        "\n",
        "            label = k if scores.labels is None else scores.labels[k]\n",
        "\n",
        "            # initial state\n",
        "            start = timestamps[0]\n",
        "            is_active = k_scores[0] > self.onset\n",
        "            curr_scores = [k_scores[0]]\n",
        "            curr_timestamps = [start]\n",
        "            for t, y in zip(timestamps[1:], k_scores[1:]):\n",
        "                # currently active\n",
        "                if is_active:\n",
        "                    curr_duration = t - start\n",
        "                    if curr_duration > self.max_duration:\n",
        "                        search_after = len(curr_scores) // 2\n",
        "                        # divide segment\n",
        "                        min_score_div_idx = search_after + np.argmin(curr_scores[search_after:])\n",
        "                        min_score_t = curr_timestamps[min_score_div_idx]\n",
        "                        region = Segment(start - self.pad_onset, min_score_t + self.pad_offset)\n",
        "                        active[region, k] = label\n",
        "                        start = curr_timestamps[min_score_div_idx]\n",
        "                        curr_scores = curr_scores[min_score_div_idx+1:]\n",
        "                        curr_timestamps = curr_timestamps[min_score_div_idx+1:]\n",
        "                    # switching from active to inactive\n",
        "                    elif y < self.offset:\n",
        "                        region = Segment(start - self.pad_onset, t + self.pad_offset)\n",
        "                        active[region, k] = label\n",
        "                        start = t\n",
        "                        is_active = False\n",
        "                        curr_scores = []\n",
        "                        curr_timestamps = []\n",
        "                    curr_scores.append(y)\n",
        "                    curr_timestamps.append(t)\n",
        "                # currently inactive\n",
        "                else:\n",
        "                    # switching from inactive to active\n",
        "                    if y > self.onset:\n",
        "                        start = t\n",
        "                        is_active = True\n",
        "\n",
        "            # if active at the end, add final region\n",
        "            if is_active:\n",
        "                region = Segment(start - self.pad_onset, t + self.pad_offset)\n",
        "                active[region, k] = label\n",
        "\n",
        "        # because of padding, some active regions might be overlapping: merge them.\n",
        "        # also: fill same speaker gaps shorter than min_duration_off\n",
        "        if self.pad_offset > 0.0 or self.pad_onset > 0.0 or self.min_duration_off > 0.0:\n",
        "            if self.max_duration < float(\"inf\"):\n",
        "                raise NotImplementedError(f\"This would break current max_duration param\")\n",
        "            active = active.support(collar=self.min_duration_off)\n",
        "\n",
        "        # remove tracks shorter than min_duration_on\n",
        "        if self.min_duration_on > 0:\n",
        "            for segment, track in list(active.itertracks()):\n",
        "                if segment.duration < self.min_duration_on:\n",
        "                    del active[segment, track]\n",
        "\n",
        "        return active\n",
        "\n",
        "\n",
        "class VoiceActivitySegmentation(VoiceActivityDetection):\n",
        "    def __init__(\n",
        "        self,\n",
        "        segmentation: PipelineModel = \"pyannote/segmentation\",\n",
        "        fscore: bool = False,\n",
        "        use_auth_token: Union[Text, None] = None,\n",
        "        **inference_kwargs,\n",
        "    ):\n",
        "\n",
        "        super().__init__(segmentation=segmentation, fscore=fscore, use_auth_token=use_auth_token, **inference_kwargs)\n",
        "\n",
        "    def apply(self, file: AudioFile, hook: Optional[Callable] = None) -> Annotation:\n",
        "        \"\"\"Apply voice activity detection\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        file : AudioFile\n",
        "            Processed file.\n",
        "        hook : callable, optional\n",
        "            Hook called after each major step of the pipeline with the following\n",
        "            signature: hook(\"step_name\", step_artefact, file=file)\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        speech : Annotation\n",
        "            Speech regions.\n",
        "        \"\"\"\n",
        "\n",
        "        # setup hook (e.g. for debugging purposes)\n",
        "        hook = self.setup_hook(file, hook=hook)\n",
        "\n",
        "        # apply segmentation model (only if needed)\n",
        "        # output shape is (num_chunks, num_frames, 1)\n",
        "        if self.training:\n",
        "            if self.CACHED_SEGMENTATION in file:\n",
        "                segmentations = file[self.CACHED_SEGMENTATION]\n",
        "            else:\n",
        "                segmentations = self._segmentation(file)\n",
        "                file[self.CACHED_SEGMENTATION] = segmentations\n",
        "        else:\n",
        "            segmentations: SlidingWindowFeature = self._segmentation(file)\n",
        "\n",
        "        return segmentations\n",
        "\n",
        "\n",
        "def merge_vad(vad_arr, pad_onset=0.0, pad_offset=0.0, min_duration_off=0.0, min_duration_on=0.0):\n",
        "\n",
        "    active = Annotation()\n",
        "    for k, vad_t in enumerate(vad_arr):\n",
        "        region = Segment(vad_t[0] - pad_onset, vad_t[1] + pad_offset)\n",
        "        active[region, k] = 1\n",
        "\n",
        "\n",
        "    if pad_offset > 0.0 or pad_onset > 0.0 or min_duration_off > 0.0:\n",
        "        active = active.support(collar=min_duration_off)\n",
        "\n",
        "    # remove tracks shorter than min_duration_on\n",
        "    if min_duration_on > 0:\n",
        "        for segment, track in list(active.itertracks()):\n",
        "            if segment.duration < min_duration_on:\n",
        "                    del active[segment, track]\n",
        "\n",
        "    active = active.for_json()\n",
        "    active_segs = pd.DataFrame([x['segment'] for x in active['content']])\n",
        "    return active_segs\n",
        "\n",
        "def merge_chunks(segments, chunk_size):\n",
        "    \"\"\"\n",
        "    Merge operation described in paper\n",
        "    \"\"\"\n",
        "    curr_end = 0\n",
        "    merged_segments = []\n",
        "    seg_idxs = []\n",
        "    speaker_idxs = []\n",
        "\n",
        "    assert chunk_size > 0\n",
        "    binarize = Binarize(max_duration=chunk_size)\n",
        "    segments = binarize(segments)\n",
        "    segments_list = []\n",
        "    for speech_turn in segments.get_timeline():\n",
        "        segments_list.append(SegmentX(speech_turn.start, speech_turn.end, \"UNKNOWN\"))\n",
        "\n",
        "    if len(segments_list) == 0:\n",
        "        print(\"No active speech found in audio\")\n",
        "        return []\n",
        "    # assert segments_list, \"segments_list is empty.\"\n",
        "    # Make sur the starting point is the start of the segment.\n",
        "    curr_start = segments_list[0].start\n",
        "\n",
        "    for seg in segments_list:\n",
        "        if seg.end - curr_start > chunk_size and curr_end-curr_start > 0:\n",
        "            merged_segments.append({\n",
        "                \"start\": curr_start,\n",
        "                \"end\": curr_end,\n",
        "                \"segments\": seg_idxs,\n",
        "            })\n",
        "            curr_start = seg.start\n",
        "            seg_idxs = []\n",
        "            speaker_idxs = []\n",
        "        curr_end = seg.end\n",
        "        seg_idxs.append((seg.start, seg.end))\n",
        "        speaker_idxs.append(seg.speaker)\n",
        "    # add final\n",
        "    merged_segments.append({\n",
        "                \"start\": curr_start,\n",
        "                \"end\": curr_end,\n",
        "                \"segments\": seg_idxs,\n",
        "            })\n",
        "    return merged_segments\n"
      ],
      "metadata": {
        "id": "XWDTWeQbQESq",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Pre Utils\n",
        "\n",
        "# Hyper Params\n",
        "def exact_div(x, y):\n",
        "    assert x % y == 0\n",
        "    return x // y\n",
        "\n",
        "SAMPLE_RATE = 16000\n",
        "N_FFT = 400\n",
        "N_MELS = 80\n",
        "HOP_LENGTH = 160\n",
        "CHUNK_LENGTH = 30\n",
        "N_SAMPLES = CHUNK_LENGTH * SAMPLE_RATE  # 480000 samples in a 30-second chunk\n",
        "N_FRAMES = exact_div(N_SAMPLES, HOP_LENGTH)  # 3000 frames in a mel spectrogram input\n",
        "\n",
        "N_SAMPLES_PER_TOKEN = HOP_LENGTH * 2  # the initial convolutions has stride 2\n",
        "FRAMES_PER_SECOND = exact_div(SAMPLE_RATE, HOP_LENGTH)  # 10ms per audio frame\n",
        "TOKENS_PER_SECOND = exact_div(SAMPLE_RATE, N_SAMPLES_PER_TOKEN)  # 20ms per audio token\n",
        "\n",
        "# functions\n",
        "!pip install -q ffmpeg-python\n",
        "import ffmpeg\n",
        "\n",
        "def data(audio, segments):\n",
        "  for seg in segments:\n",
        "      f1 = int(seg['start'] * SAMPLE_RATE)\n",
        "      f2 = int(seg['end'] * SAMPLE_RATE)\n",
        "      # print(f2-f1)\n",
        "      yield {'inputs': audio[f1:f2]}\n",
        "\n",
        "def load_audio(file: str, sr: int = SAMPLE_RATE):\n",
        "    \"\"\"Open an audio file and read as mono waveform, resampling as necessary\"\"\"\n",
        "    try:\n",
        "        # This launches a subprocess to decode audio while down-mixing and resampling as necessary.\n",
        "        # Requires the ffmpeg CLI and `ffmpeg-python` package to be installed.\n",
        "        out, _ = (\n",
        "            ffmpeg.input(file, threads=0)\n",
        "            .output(\"-\", format=\"s16le\", acodec=\"pcm_s16le\", ac=1, ar=sr)\n",
        "            .run(cmd=[\"ffmpeg\", \"-nostdin\"], capture_stdout=True, capture_stderr=True)\n",
        "        )\n",
        "    except ffmpeg.Error as e:\n",
        "        raise RuntimeError(f\"Failed to load audio: {e.stderr.decode()}\") from e\n",
        "\n",
        "    return np.frombuffer(out, np.int16).flatten().astype(np.float32) / 32768.0"
      ],
      "metadata": {
        "id": "O_qj_QulRSVy",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Download 30 min Segment of Video into Source.wav\n",
        "!pip install -q -U yt-dlp moviepy\n",
        "\n",
        "#Create Audio from Local Video {Download Video Still to come}\n",
        "from moviepy.editor import VideoFileClip\n",
        "import os\n",
        "## Amharic Dataset Preparation Pipeline {30 min}\n",
        "SOURCE_VIDEO = 'temp.mp4'\n",
        "SOURCE_AUDIO = 'source.wav'\n",
        "\n",
        "# Set the URL of the video to download & Download the video\n",
        "video_url = 'https://www.youtube.com/watch?v=r10iazydyHY' #@param {type:\"string\"}\n",
        "!yt-dlp -o \"temp.mp4\" -f \"bestvideo[height<=1080][ext=mp4]+bestaudio[ext=m4a]/best[ext=mp4]/best\" {video_url}\n",
        "\n",
        "\n",
        "SAVE_DOWNLOADED = 'data'\n",
        "os.makedirs(SAVE_DOWNLOADED,exist_ok=True)\n",
        "\n",
        "\n",
        "# Extract audio from the temporary video file and save as source.wav\n",
        "video_clip = VideoFileClip(SOURCE_VIDEO)\n",
        "audio_clip = video_clip.audio\n",
        "audio_clip.subclip(0, 1800).write_audiofile(SOURCE_AUDIO,\n",
        "                            fps=16000,\n",
        "                            nbytes=2,\n",
        "                            bitrate='16k',\n",
        "                            codec='pcm_s16le')\n",
        "\n",
        "# # Clean up temporary video file\n",
        "# os.remove(SOURCE_VIDEO)\n",
        "\n",
        "print('Download and audio extraction completed successfully.')"
      ],
      "metadata": {
        "cellView": "form",
        "id": "Ex8hyIwMgjAr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Get Json of 30 Second Chunks\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "audio = load_audio('/content/source.wav')\n",
        "\n",
        "default_vad_options = {\"vad_onset\": 0.500,\"vad_offset\": 0.363}\n",
        "vad_model = load_vad_model(torch.device(device), use_auth_token=None, **default_vad_options)\n",
        "\n",
        "vad_segments = vad_model({\"waveform\": torch.from_numpy(audio).unsqueeze(0), \"sample_rate\": SAMPLE_RATE})\n",
        "vad_segments = merge_chunks(vad_segments, 30)\n",
        "\n",
        "# Saving For Local Proccessing\n",
        "import json\n",
        "open('vad.json','w').write(json.dumps(vad_segments))"
      ],
      "metadata": {
        "id": "AxfomwXDQWpl",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "_**ALL DOCS.GOOGLE.COM goes here**_  \n",
        "Now Download `vad.json` && `source.wav` and run local code with docs.google.com  \n",
        "It returns {30 second texted vad segments}  \n",
        "[  \n",
        "  {'text': \"Like So\", 'start': 0.008, 'end': 27.11},   \n",
        "{'text': \" for each 30 seconds\", 'start': 28.645, 'end': 55.595}  \n",
        "]  \n",
        "saved as `segments.json`, this is from where we continue  \n",
        "re-run all code expept get 30-second segment to continue"
      ],
      "metadata": {
        "id": "TebCFC5s5z6e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Continue Long Formation\n",
        "# fixing adjustment for amharic\n",
        "pre = \"\"\"\n",
        "    \"ja\": \"jonatasgrosman/wav2vec2-large-xlsr-53-japanese\",\"\"\"\n",
        "post = \"\"\"\n",
        "    \"ja\": \"jonatasgrosman/wav2vec2-large-xlsr-53-japanese\",\n",
        "    \"am\": \"agkphysics/wav2vec2-large-xlsr-53-amharic\",\"\"\"\n",
        "def find_and_replace(file_path, find, replace):\n",
        "    with open(file_path) as f:\n",
        "        s = f.read()\n",
        "        s = s.replace(find, replace)\n",
        "    with open(file_path, \"w\") as f:\n",
        "        f.write(s)\n",
        "file_path = '/usr/local/lib/python3.10/dist-packages/whisperx/alignment.py'\n",
        "find_and_replace(file_path, pre, post)\n",
        "\n",
        "# load whixperx and Perfect timestamp alignment\n",
        "import json\n",
        "result = json.loads(open('segments.json','r').read())\n",
        "result\n",
        "import whisperx\n",
        "import torch\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "# 2. Align whisper output\n",
        "model_a, metadata = whisperx.load_align_model(language_code='am', device=device)\n",
        "result = whisperx.align(result[\"segments\"], model_a, metadata, audio, device, return_char_alignments=False)\n",
        "\n",
        "# 3. Assign speaker labels\n",
        "diarize_model = whisperx.DiarizationPipeline(use_auth_token='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', device=device)\n",
        "\n",
        "# add min/max number of speakers if known\n",
        "diarize_segments = diarize_model(SOURCE_AUDIO)\n",
        "# diarize_model(audio_file, min_speakers=min_speakers, max_speakers=max_speakers) # You Desire to specify\n",
        "\n",
        "result1 = whisperx.assign_word_speakers(diarize_segments, result)\n",
        "print(diarize_segments)\n",
        "print(result1[\"segments\"]) # segments are now assigned speaker IDs"
      ],
      "metadata": {
        "cellView": "form",
        "id": "9mU2lrpS7X5l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Download, Dataset in User IDentified and Speech Only\n",
        "# this Json has Clean Text-Speech pair & with User Identification\n",
        "import json\n",
        "open('clean_segments.json','w').write(json.dumps(result[\"segments\"]))\n",
        "open('speaker_segments.json','w').write(json.dumps(result1[\"segments\"]))"
      ],
      "metadata": {
        "cellView": "form",
        "id": "fVgHmHAj6au3"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}